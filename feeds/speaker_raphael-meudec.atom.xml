<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="/" rel="alternate"></link><link href="/feeds/speaker_raphael-meudec.atom.xml" rel="self"></link><id>/</id><updated>2019-11-05T00:00:00+00:00</updated><entry><title>tf-explain: Interpretability for Tensorflow 2.0</title><link href="/pydata-new-york-city-2019/tf-explain-interpretability-for-tensorflow-20.html" rel="alternate"></link><published>2019-11-05T00:00:00+00:00</published><updated>2019-11-05T00:00:00+00:00</updated><author><name>RaphaÃ«l Meudec</name></author><id>tag:,2019-11-05:pydata-new-york-city-2019/tf-explain-interpretability-for-tensorflow-20.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Deep learning models now emerge in multiple domains. The question data scientists and users always ask is 'Why does it work?'. Explaining decisions from neural networks is vital for model improvements and analysis, and users' adoption. In this talk, I will explain interpretability methods implementations with TF2.0 and introduce tf-explain, a TF2.0 library for interpretability.&lt;/p&gt;
</summary></entry></feed>