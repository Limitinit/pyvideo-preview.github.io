<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="/" rel="alternate"></link><link href="/feeds/speaker_matthew-honnibal.atom.xml" rel="self"></link><id>/</id><updated>2019-10-12T00:00:00+00:00</updated><entry><title>Building new NLP solutions with spaCy and Prodigy</title><link href="/europython-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html" rel="alternate"></link><published>2018-07-26T00:00:00+00:00</published><updated>2018-07-26T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:,2018-07-26:europython-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Commercial machine learning projects are currently like start-ups: many
projects fail, but some are extremely successful, justifying the total
investment. While some people will tell you to “embrace failure”, I say
failure sucks — so what can we do to fight it? In this talk, I will
discuss how to address some of the most likely causes of failure for new
Natural Language Processing (NLP) projects. My main recommendation is to
take an iterative approach: don’t assume you know what your pipeline
should look like, let alone your annotation schemes or model
architectures. I will also discuss a few tips for figuring out what’s
likely to work, along with a few common mistakes. To keep the advice
well-grounded, I will refer specifically to our open-source library
spaCy, and our commercial annotation tool Prodigy.&lt;/p&gt;
</summary></entry><entry><title>spaCy PyTorch Transformers - Matthew Honnibal</title><link href="/pycon-india-2019/spacy-pytorch-transformers-matthew-honnibal.html" rel="alternate"></link><published>2019-10-12T00:00:00+00:00</published><updated>2019-10-12T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:,2019-10-12:pycon-india-2019/spacy-pytorch-transformers-matthew-honnibal.html</id><summary type="html"></summary></entry><entry><title>Why Python's the best language for AI (and how to make it even better) -</title><link href="/pycon-israel-2017/why-pythons-the-best-language-for-ai-and-how-to-make-it-even-better-.html" rel="alternate"></link><published>2017-06-13T00:00:00+00:00</published><updated>2017-06-13T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:,2017-06-13:pycon-israel-2017/why-pythons-the-best-language-for-ai-and-how-to-make-it-even-better-.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Over the last few years, Python has cemented its lead as the primary development language for machine learning and AI, among both researchers and engineers. Python is dominating this development niche because manual memory management is very important for numeric computing, and CPython exposes an efficient and practical API for low-level extensions. The rest of the Python ecosystem also relies on low-level extensions --- Python would be of little use for web programming without fast libraries for JSON and XML parsing, image manipulation and database connectivity. However, most Python users have little love for these native extensions that they rely on. Native extensions impose some inconveniences. More things can go wrong, and they're necessarily exceptional and less dynamic.In this talk, I'll argue that the Python community ought to accept native extensions as a necessary good, instead of a necessary evil. The language would get stronger at its strengths by ending its ambivalence about native extensions, and investing in better tooling. Individual developers would benefit from getting comfortable dropping down into Cython, and understanding how to read and build native libraries. This applies especially in the growing field of machine learning and artificial intelligence --- the biggest growth area in software development today.&lt;/p&gt;
</summary><category term="keynote"></category></entry><entry><title>Designing spaCy: Industrial-strength NLP</title><link href="/pydata-berlin-2016/designing-spacy-industrial-strength-nlp.html" rel="alternate"></link><published>2016-05-31T00:00:00+00:00</published><updated>2016-05-31T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:,2016-05-31:pydata-berlin-2016/designing-spacy-industrial-strength-nlp.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;The spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. This talk describes how we’ve met these challenges in spaCy, by implementing the library in Cython.&lt;/p&gt;
&lt;p&gt;The spaCy natural language processing (NLP) library features state-of-the-art performance, and a high-level Python API. Efficiency is crucial for NLP, because job sizes are constantly increasing. The key algorithms are also relatively complicated, and frequently subject to change, as new research is published. This talk describes how we’ve met these challenges in spaCy, by implementing the library in Cython. Unlike many Cython users, we did not write the library in Python first, and then optimize it. Instead, we designed the library as a C extension from the start, and added the Python API on top. This allows us to build the library on top of efficient, memory-managed data structures, without having to maintain a separate C or C++ codebase. The result is the fastest NLP library in the world, support for GIL-free multithreading, in a concise readable codebase, and with no compromise on user friendliness.&lt;/p&gt;
</summary><category term="spacy"></category><category term="npl"></category></entry><entry><title>Building new NLP solutions with spaCy and Prodigy</title><link href="/pydata-berlin-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html" rel="alternate"></link><published>2018-07-07T00:00:00+00:00</published><updated>2018-07-07T00:00:00+00:00</updated><author><name>Matthew Honnibal</name></author><id>tag:,2018-07-07:pydata-berlin-2018/building-new-nlp-solutions-with-spacy-and-prodigy.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this talk, I will discuss how to address some of the most likely
causes of failure for new Natural Language Processing (NLP) projects. My
main recommendation is to take an iterative approach: don't assume you
know what your pipeline should look like, let alone your annotation
schemes or model architectures.&lt;/p&gt;
</summary></entry></feed>