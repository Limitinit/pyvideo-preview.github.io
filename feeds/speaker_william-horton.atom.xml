<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="/" rel="alternate"></link><link href="/feeds/speaker_william-horton.atom.xml" rel="self"></link><id>/</id><updated>2020-07-23T00:00:00+00:00</updated><entry><title>A Brief History of Jupyter Notebooks</title><link href="/europython-2020/a-brief-history-of-jupyter-notebooks.html" rel="alternate"></link><published>2020-07-23T00:00:00+00:00</published><updated>2020-07-23T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:,2020-07-23:europython-2020/a-brief-history-of-jupyter-notebooks.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;The Jupyter Notebook: many Python users love it, many other Python users love to hate it. But where did it come from? How did we come to have a tool that combines code execution, visualization, Markdown, and more? In this talk, we will dive into the development of the Jupyter Notebook and the older ideas that it built upon.&lt;/p&gt;
&lt;p&gt;To start, we will look at tools that popularized the “computational notebook” interface. In 1988, Mathematica introduced this interface to the scientific community. In the 90s, tools like Maple competed with Mathematica to provide the best scientific programming environment. The early 2000s saw the rise in popularity of open-source scientific tools in Python, including IPython, leading to IPython Notebook and then Jupyter.&lt;/p&gt;
&lt;p&gt;Turning to the present, we look at the expanding ecosystem beyond the Notebook. JupyterLab provides a richer programming environment. Voilà and Binder give users better options for sharing their notebooks. And increased language support has led to Jupyter being a tool not only for Julia, Python, and R, but for dozens of other languages.&lt;/p&gt;
&lt;p&gt;Finally: what is still to come? JupyerLab 2.0 promises even greater IDE-like capabilities, while IDEs increase their own Notebook support. Projects like Deepnote and CoCalc promise real-time collaboration on top of the Notebook interface. And the frustrations of working with Git are the source of a growing number of possible solutions. These efforts point us toward what the Jupyter Notebook could become.&lt;/p&gt;
</summary><category term="europython"></category><category term="europython-2020"></category><category term="europython-online"></category><category term="Data Science"></category><category term="Ipython"></category><category term="Jupyter"></category><category term="Open-Source"></category><category term="Scientific Libraries (Numpy/Pandas/SciKit/...)"></category></entry><entry><title>CUDA in your Python: Parallel Programming on the GPU</title><link href="/pybay-2019/cuda-in-your-python-parallel-programming-on-the-gpu.html" rel="alternate"></link><published>2019-08-17T00:00:00+00:00</published><updated>2019-08-17T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:,2019-08-17:pybay-2019/cuda-in-your-python-parallel-programming-on-the-gpu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk was presented at PyBay2019 - 4th annual Bay Area Regional Python conference. See pybay.com for more details about PyBay and click SHOW MORE for more information about this talk.&lt;/p&gt;
&lt;p&gt;Description
It’s 2019, and Moore’s Law is dead. CPU performance is plateauing, but GPUs provide a chance for continued hardware performance gains, if you can structure your programs to make good use of them. In this talk you will learn how to speed up your Python programs using Nvidia’s CUDA platform.&lt;/p&gt;
&lt;p&gt;Abstract
CUDA is a platform developed by Nvidia for GPGPU--general purpose computing with GPUs. It backs some of the most popular deep learning libraries, like Tensorflow and Pytorch, but it has broader uses in data analysis, data science, and machine learning.&lt;/p&gt;
&lt;p&gt;There are several ways that you can start taking advantage of CUDA in your Python programs.&lt;/p&gt;
&lt;p&gt;For some common Python libraries, there are drop-in replacements that let you start running computations on the GPU, while still using APIs that you might be familiar with. For example, CuPy provides a Numpy-like API for interacting with multi-dimensional arrays. Another recent project is cuDF by RAPIDS AI, which mimics the pandas interface for dataframes.&lt;/p&gt;
&lt;p&gt;If you want more control over your use of CUDA APIs, you can use the PyCUDA library, which provides bindings for the CUDA API that you can call from your Python code. Compared with drop-in libraries, it gives you the ability to manually allocate memory on the GPU, as well as to write custom CUDA code. However, it comes with some drawbacks, such as having to write your CUDA code as large strings in your Python program, and compiling your CUDA code while running your program, rather than beforehand.&lt;/p&gt;
&lt;p&gt;Finally, for the best performance you can use the Python C/C++ extension interface, the approach taken by deep learning libraries like Pytorch. One of the strengths of Python is the ability to drop down into C/C++, and libraries like Numpy take advantage of this for increased speed. If you use Nvidia’s nvcc compiler for CUDA, you can use the same extension interface to write custom programs in CUDA and then call them from your Python code.&lt;/p&gt;
&lt;p&gt;This talk will explore each of these methods, provide examples to get started, and discuss in more detail the pros and cons of each approach.&lt;/p&gt;
&lt;p&gt;About the speaker
William Horton is a Senior Backend Engineer at Compass, where he works on systems for ingesting, processing, and serving millions of real estate listings. In his spare time, he blogs and speaks about deep learning, contributes to open-source libraries like fastai and pytorch, and competes in computer vision competitions on Kaggle. When he’s not doing tech things, he enjoys powerlifting and singing a cappella.&lt;/p&gt;
&lt;p&gt;Sponsor Acknowledgement
This and other PyBay2019 videos are via the help of our media partner AlphaVoice (&lt;a class="reference external" href="https://www.alphavoice.io/"&gt;https://www.alphavoice.io/&lt;/a&gt;)!&lt;/p&gt;
&lt;p&gt;#pybay #pybay2019 #python #python3 #gdb&lt;/p&gt;
</summary></entry><entry><title>CUDA in Your Python: Effective Parallel Programming on the GPU</title><link href="/pycolorado-2019/cuda-in-your-python-effective-parallel-programming-on-the-gpu.html" rel="alternate"></link><published>2019-09-08T00:00:00+00:00</published><updated>2019-09-08T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:,2019-09-08:pycolorado-2019/cuda-in-your-python-effective-parallel-programming-on-the-gpu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It’s 2019, and Moore’s Law is dead. CPU performance is plateauing, but GPUs provide a chance for continued hardware performance gains, if you can structure your programs to make good use of them. In this talk you will learn how to speed up your Python programs using Nvidia’s CUDA platform.&lt;/p&gt;
</summary></entry><entry><title>CUDA in your Python: Effective Parallel Programming on the GPU</title><link href="/pycon-us-2019/cuda-in-your-python-effective-parallel-programming-on-the-gpu.html" rel="alternate"></link><published>2019-05-04T17:10:00+00:00</published><updated>2019-05-04T17:10:00+00:00</updated><author><name>William Horton</name></author><id>tag:,2019-05-04:pycon-us-2019/cuda-in-your-python-effective-parallel-programming-on-the-gpu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It’s 2019, and Moore’s Law is dead. CPU performance is plateauing, but
GPUs provide a chance for continued hardware performance gains, if you
can structure your programs to make good use of them.&lt;/p&gt;
&lt;p&gt;CUDA is a platform developed by Nvidia for GPGPU--general purpose
computing with GPUs. It backs some of the most popular deep learning
libraries, like Tensorflow and Pytorch, but has broader uses in data
analysis, data science, and machine learning.&lt;/p&gt;
&lt;p&gt;There are several ways that you can start taking advantage of CUDA in
your Python programs.&lt;/p&gt;
&lt;p&gt;For some common Python libraries, there are drop-in replacements that
let you start running computations on the GPU while still using familiar
APIs. For example, CuPy provides a NumPy-like API for interacting with
multi-dimensional arrays. Similarly, cuDF is a recent project that
mimics the pandas interface for dataframes.&lt;/p&gt;
&lt;p&gt;If you want more control over your use of CUDA APIs, you can use the
PyCUDA library, which provides bindings for the CUDA API that you can
call from your Python code. Compared with drop-in libraries, it gives
you the ability to manually allocate memory on the GPU, and write custom
CUDA functions (called kernels). However, its drawbacks include writing
your CUDA code as large strings in Python, and compiling your CUDA code
at runtime.&lt;/p&gt;
&lt;p&gt;Finally, for the best performance you can use the Python C/C++ extension
interface, the approach taken by deep learning libraries like Pytorch.
One of the strengths of Python is the ability to drop down into C/C++,
and libraries like NumPy take advantage of this for increased speed. If
you use Nvidia’s nvcc compiler for CUDA, you can use the same extension
interface to write custom CUDA kernels, and then call them from your
Python code.&lt;/p&gt;
&lt;p&gt;This talk will explore each of these methods, provide examples to get
started, and discuss in more detail the pros and cons of each approach.&lt;/p&gt;
</summary><category term="talk"></category></entry><entry><title>You Can Do Deep Learning!</title><link href="/pyohio-2018/you-can-do-deep-learning.html" rel="alternate"></link><published>2018-07-28T00:00:00+00:00</published><updated>2018-07-28T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:,2018-07-28:pyohio-2018/you-can-do-deep-learning.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;When I started learning web dev, I dove into building my first Rails
app. I didn’t know how it all worked, but after hours of hacking I had a
blogging app running. I imagine many share a similar learning
experience. Similarly, you don’t need a PhD to do deep learning, you can
get started with Python skills and open-source frameworks. It can be fun
and rewarding, and inspire you to dive deeper.&lt;/p&gt;
</summary></entry><entry><title>CUDA in your Python: Effective Parallel Programming on the GPU</title><link href="/pytexas-2019/cuda-in-your-python-effective-parallel-programming-on-the-gpu.html" rel="alternate"></link><published>2019-04-13T00:00:00+00:00</published><updated>2019-04-13T00:00:00+00:00</updated><author><name>William Horton</name></author><id>tag:,2019-04-13:pytexas-2019/cuda-in-your-python-effective-parallel-programming-on-the-gpu.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;It’s 2019, and Moore’s Law is dead. CPU performance is plateauing, but GPUs provide a chance for continued hardware performance gains, if you can structure your programs to make good use of them. In this talk you will learn how to speed up your Python programs using Nvidia’s CUDA platform.&lt;/p&gt;
</summary><category term="GPU"></category><category term="cuda"></category></entry></feed>